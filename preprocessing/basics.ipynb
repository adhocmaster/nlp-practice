{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66473dd7-ac4f-41bf-9544-6fde7a287cae",
   "metadata": {},
   "source": [
    "# Vectorization and preprocessing\n",
    "### 1. Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c35cbfb-d987-465d-aeec-16c5e5ac22d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\adhocmaster\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13fefd45-31fb-4404-90c7-70b5bed495ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4d6216-803b-4aae-afc6-f09e7ad40cee",
   "metadata": {},
   "source": [
    "### 2. Stemming and Lemmatization\n",
    "\n",
    "Stemming chops off the end of the world, lemmatization finds the root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c839d52-fa16-45b3-a3eb-da22872d1b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['walk', 'walk', 'walk']\n",
      "['go', 'go', 'went']\n",
      "['good', 'better', 'best']\n",
      "['is', 'wa']\n",
      "['mous', 'mice']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "print([porter.stem(word) for word in [\"walking\", \"walked\", \"walks\"]])\n",
    "print([porter.stem(word) for word in [\"go\", \"going\", \"went\"]])\n",
    "print([porter.stem(word) for word in [\"good\", \"better\", \"best\"]])\n",
    "print([porter.stem(word) for word in [\"is\", \"was\"]])\n",
    "print([porter.stem(word) for word in [\"mouse\", \"mice\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed57f229-29c2-4df5-a82b-770eb07eb0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['walking', 'walked', 'walk']\n",
      "['walk', 'walk', 'walk']\n",
      "['go', 'going', 'went']\n",
      "['go', 'go', 'go']\n",
      "['good', 'better', 'best']\n",
      "['good', 'good', 'best']\n",
      "['is', 'wa']\n",
      "['be', 'be']\n",
      "['mouse', 'mouse']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "# nltk.download(\"wordnet\")\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print([lemmatizer.lemmatize(word) for word in [\"walking\", \"walked\", \"walks\"]])\n",
    "print([lemmatizer.lemmatize(word, wordnet.VERB) for word in [\"walking\", \"walked\", \"walks\"]])\n",
    "print([lemmatizer.lemmatize(word) for word in [\"go\", \"going\", \"went\"]])\n",
    "print([lemmatizer.lemmatize(word, wordnet.VERB) for word in [\"go\", \"going\", \"went\"]])\n",
    "print([lemmatizer.lemmatize(word) for word in [\"good\", \"better\", \"best\"]])\n",
    "print([lemmatizer.lemmatize(word, wordnet.ADJ) for word in [\"good\", \"better\", \"best\"]])\n",
    "print([lemmatizer.lemmatize(word) for word in [\"is\", \"was\"]])\n",
    "print([lemmatizer.lemmatize(word, wordnet.VERB) for word in [\"is\", \"was\"]])\n",
    "print([lemmatizer.lemmatize(word) for word in [\"mouse\", \"mice\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc32b73-5615-47e2-b99b-80165d7494c3",
   "metadata": {},
   "source": [
    "### 3. POST Tagging\n",
    "To correctly lemmatize, we need POS Tagging first. But the tags used by nltk pos tagger is not compatible with the tags that WordNetLemmatizer use. So, we need a mapping. Here goes the mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5e52363-7bc3-483e-bca9-75bc198a540d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\adhocmaster\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28a5fa6f-f3d9-4d69-b503-45ebfc371eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "def treebankToWordnetPOS(treebankTag: str) -> str:\n",
    "    if treebankTag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    if treebankTag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    if treebankTag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    if treebankTag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57f3d157-c85b-49f7-9a56-21574f08b7ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Alexis', 'NNP'),\n",
       " ('Mac', 'NNP'),\n",
       " ('Allister', 'NNP'),\n",
       " ('scored', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('opening', 'NN'),\n",
       " ('goal', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('game,', 'NN'),\n",
       " ('after', 'IN'),\n",
       " ('Wojciech', 'NNP'),\n",
       " ('Szczesny', 'NNP'),\n",
       " ('denied', 'VBD'),\n",
       " ('Lionel', 'NNP'),\n",
       " ('Messi', 'NNP'),\n",
       " ('from', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('penalty', 'NN'),\n",
       " ('spot', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('brilliant', 'JJ'),\n",
       " ('spot.', 'NN')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Alexis Mac Allister scored the opening goal of the game, after Wojciech Szczesny denied Lionel Messi from the penalty spot with a brilliant spot.\"\n",
    "wordsAndTags = nltk.pos_tag(sentence.split())\n",
    "wordsAndTags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11bf0892-4eb3-4c30-89bd-444e576f5df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alexis', 'Mac', 'Allister', 'score', 'the', 'opening', 'goal', 'of', 'the', 'game,', 'after', 'Wojciech', 'Szczesny', 'deny', 'Lionel', 'Messi', 'from', 'the', 'penalty', 'spot', 'with', 'a', 'brilliant', 'spot.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print([lemmatizer.lemmatize(word, treebankToWordnetPOS(tbTag)) for word, tbTag in wordsAndTags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa94b80-8e74-45dc-b2ad-1e8dbf08c162",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
