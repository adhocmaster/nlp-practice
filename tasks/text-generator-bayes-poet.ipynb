{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0519a25-0d76-44c0-9a25-79233abd054b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working dirD:\\ML\\nlp\\nlp-practice\n"
     ]
    }
   ],
   "source": [
    "exec(open('init_notebook.py').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3a11de1-efd3-48ce-b3ed-0da46441f370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "from sklearn.metrics import RocCurveDisplay, roc_curve\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "\n",
    "from nltk import word_tokenize\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7146e9d-59c2-45f8-bc95-0bd3d0d700f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:/ML/nlp/nlp-practice\\\\data'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDir = os.path.join(projectFolder, \"data\")\n",
    "dataDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92fde756-470b-4c45-99d2-974746bd8fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url1 = \"https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/edgar_allan_poe.txt\"\n",
    "# url2 = \"https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/robert_frost.txt\"\n",
    "# wget.download(url1, out=dataDir)\n",
    "# wget.download(url2, out=dataDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ec4400-ec8d-4efe-9391-da42321c3d14",
   "metadata": {},
   "source": [
    "# Task = Given a Poem, who is the poet?\n",
    "## Model using a Bayes-Classifier.\n",
    "\n",
    "p(x|author1), p(x|author2) can be calculated by creating two markov models, one for author1 and one for author2. So, we can use unsupervised method for supervised task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbcec830-31cf-4bf4-b876-e6b7f158fe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from library.common_preprocess import *\n",
    "from library.selection import *\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab53e1a1-eef0-42fd-8817-11b4576aa0be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42416f79-8a87-45c5-848b-f7b7e705f3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dataDir, \"edgar_allan_poe.txt\")) as f:\n",
    "    poeSentences = f.readlines()\n",
    "with open(os.path.join(dataDir, \"robert_frost.txt\")) as f:\n",
    "    frostSentences = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa31406d-2667-4d4a-989b-dbf2be34a740",
   "metadata": {},
   "source": [
    "## Step 1: Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c28e718-5652-4f87-84fa-6da9426268ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "poeSentences = lemmatize(poeSentences)\n",
    "frostSentences = lemmatize(frostSentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d8aa95-2180-4c14-af2c-f5170b485769",
   "metadata": {},
   "source": [
    "## Step 2: train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a16ca29-4ce1-45d9-9dee-698430c0498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "poTrain, poTest = split(poeSentences, 0.7)\n",
    "frostTrain, frostTest = split(frostSentences, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098b9ddf-df14-4294-9905-dc618b5eeba8",
   "metadata": {},
   "source": [
    "## Step 3: Build vocabulary based on the train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22505b95-41a8-4d2b-8247-cd9f0ed97588",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet = poTrain + frostTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3909aecf-dce1-406e-95a0-09c95d7ef79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** top 10 words out of 1958 words***\n",
      "the 599\n",
      "and 306\n",
      "be 301\n",
      "i 280\n",
      "a 275\n",
      "of 268\n",
      "to 263\n",
      "in 209\n",
      "it 179\n",
      "that 157\n"
     ]
    }
   ],
   "source": [
    "wordToIndex = buildWordToIndex (\n",
    "    word_tokenize, \n",
    "    trainSet, \n",
    "    maxSize = 5000,\n",
    "    # stopWords=stopwords.words('english'),\n",
    "    stopWords=None,\n",
    "    lowercase=True,\n",
    "    ignorePunkt=True\n",
    ")\n",
    "idxToWord = {v: k for k, v in wordToIndex.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df1c4002-3c91-40f8-83b6-5ab91de192e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1959"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabularySize = len(wordToIndex)\n",
    "vocabularySize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654435f0-f131-4c85-8de2-d42e531a27f5",
   "metadata": {},
   "source": [
    "## Step 4: tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cb8c27f-760f-4d58-9e2e-023876bb0bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "poTrainTokens = [tokenizeDoc(word_tokenize, doc, wordToIndex) for doc in poTrain]\n",
    "frostTrainTokens = [tokenizeDoc(word_tokenize, doc, wordToIndex) for doc in frostTrain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52ceda01-2987-441d-a224-22c320c19848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 4, 233, 557, 15, 420]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poTrainTokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50f995bb-ce0a-4814-b711-163b7cd4fb63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In a strange city , all alone ,'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poTrain[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f854b083-7147-489a-967f-ebb2639fe677",
   "metadata": {},
   "source": [
    "## Step 5: Build Markov Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb4c8421-139f-45ae-9a7e-90938c6f03ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "TransitionMatrix = npt.NDArray\n",
    "MarginalProbabilities = npt.NDArray\n",
    "Probablity = float\n",
    "\n",
    "# Markov Model for an author\n",
    "def buildMarkovModel(sentences: List[List[Token]], vocabularySize: int) -> Tuple[TransitionMatrix, MarginalProbabilities]:\n",
    "    # step 1: initialize A, and pi\n",
    "    \n",
    "    # Initialize with Add-One Smoothing or a very low value\n",
    "    pi = np.ones(vocabularySize)\n",
    "    A = np.ones((vocabularySize, vocabularySize))\n",
    "    \n",
    "    # print(vocabularySize)\n",
    "    for aSenTokens in sentences:\n",
    "        # print(aSenTokens)\n",
    "        if len(aSenTokens) > 0:\n",
    "            pi[aSenTokens[0]] += 1 # initial state\n",
    "            for i in range(1, len(aSenTokens)): # i next, j is previous.\n",
    "                iToken = aSenTokens[i]\n",
    "                jToken = aSenTokens[i-1]\n",
    "                A[iToken, jToken] += 1\n",
    "                \n",
    "    # now each column in A represents a previous word. So, we normalize the column to get probabilities P(i|j).\n",
    "    \n",
    "    pi /= np.sum(pi)\n",
    "    A /= np.sum(A, axis=0)\n",
    "    \n",
    "    # followin is the validation check if each column adds up to 1\n",
    "    # Asums = np.sum(A, axis=0)\n",
    "    # ones = np.ones_like(Asums)\n",
    "    # for col in range(len(Asums)):\n",
    "    #     assert np.allclose(Asums, ones)\n",
    "    \n",
    "    return A, pi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e69e9823-c314-4336-b3f5-194a9a7fd30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "poA, poPi = buildMarkovModel(poTrainTokens, vocabularySize)\n",
    "frA, frPi = buildMarkovModel(frostTrainTokens, vocabularySize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397377ca-7eec-44e4-a310-1f458177feba",
   "metadata": {},
   "source": [
    "## Step 6: Bayes Model - Generative Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd7710a4-051f-419d-8f5a-6d99c9601762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It does not work well as it just learns bi-grams.\n",
    "\n",
    "class GenModel:\n",
    "    def __init__(self, \n",
    "                 indexToWord: Dict[Token, WordRoot],  \n",
    "                 A1: TransitionMatrix, \n",
    "                 pi1: MarginalProbabilities, \n",
    "                 A2: TransitionMatrix, \n",
    "                 pi2: MarginalProbabilities\n",
    "        ):\n",
    "        self.indexToWord = indexToWord\n",
    "        \n",
    "        self.mdps = {\n",
    "            1: [A1, pi1],\n",
    "            2: [A2, pi2]\n",
    "        }\n",
    "        \n",
    "    \n",
    "    def __call__(self, label: Label, n: int) -> Text:\n",
    "        # 1. sample n tokens\n",
    "        # 2. convert tokens to words\n",
    "        \n",
    "        A, pi = self.mdps[label]\n",
    "        allTokens = list(range(len(poPi)))\n",
    "        \n",
    "        # 1. \n",
    "        txtBuffer = []\n",
    "        firstToken = np.random.choice(allTokens, p=poPi)\n",
    "        txtBuffer.append(self.indexToWord[firstToken]) # our first word root!\n",
    "        \n",
    "        prevToken = firstToken\n",
    "        for i in range(1, n):\n",
    "            nextTokenProbs = A[:, prevToken]\n",
    "            nextToken = np.random.choice(allTokens, p=nextTokenProbs)\n",
    "            txtBuffer.append(self.indexToWord[nextToken])\n",
    "        return \" \". join(txtBuffer)\n",
    "            \n",
    "        \n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63359dee-7396-48af-aa74-a68193262e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GenModel(\n",
    "        indexToWord=idxToWord,\n",
    "        A1=poA,\n",
    "        pi1=poPi,\n",
    "        A2=frA,\n",
    "        pi2=frPi\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe8854f7-fd05-47dc-b105-f12e2e2b3986",
   "metadata": {},
   "outputs": [],
   "source": [
    "poLabel = 1\n",
    "frLabel = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "398afe35-d11c-45e6-b91a-f1a569c10e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******seed************************************ 0\n",
      "his glory birth spirit spirit birth spirit undivided wandering spirit\n",
      "is owe owe waste the the the waste waste waste\n",
      "pauses in in in in in in in in in\n",
      "in tear her some the good good good couple thine\n",
      "that night hang the hang hang i be on my\n",
      "where no israfel the the the deep the no the\n",
      "i be journey i see know wake be roam be\n",
      "blushes with with with with with with with with with\n",
      "it may be duplicate grow not own be grow ash\n",
      "it doth be duplicate not be grow be thrive be\n",
      "thy memory holy peer memory presence presence presence memory peer\n",
      "they follow trail be follow would plunge trail plunge trail\n",
      "through the two an the an the the an the\n",
      "their eternal bed hymn echoing merry echoing melancholy melancholy eternal\n",
      "*******seed************************************ 1\n",
      "but guide the they to a that where now light\n",
      "not leave the linger the like that with the the\n",
      "these rose cheek rose rose rose cheek cheek cheek rose\n",
      "and now wisdom star palace not palace conquer the sigh\n",
      "t awake awake awake awake awake awake awake awake awake\n",
      "the fervor sky right misty year tremble star lava upturn\n",
      "and now palace now the star tempt wo stately thou\n",
      "said sadly sadly sadly sadly sadly sadly sadly sadly sadly\n",
      "better than than than than than than than than than\n",
      "and ruby then grow no that sanctify grow the sere\n",
      "that town ungodly my crowd while the israfeli hang blush\n",
      "i i heed journey be mistrust mistrust heed care i\n",
      "the night night leaf pearly vista sad gorgeous long dead\n",
      "at length once the noon the noon a noon sight\n",
      "*******seed************************************ 2\n",
      "on the my my my a the this a a\n",
      "their steel bed steel bright electric elysian steel elysian heart\n",
      "by the the the the their the the the the\n",
      "from the their out out her your their the their\n",
      "sought a a a a a a a a a\n",
      "like the some starlight those the a the warrior warrior\n",
      "these rose rose cheek cheek rose cheek cheek rose cheek\n",
      "by the the the thee our the the which the\n",
      "at the least a thy thy a once length the\n",
      "it be duplicate doth be may need doth be power\n",
      "hath ever striven striven striven beat ever dwelt beat palsy\n",
      "in secret that their heaven agony voice the heaven his\n",
      "his birth voice heart heart voice home birth voice heart\n",
      "and all in all yet thine brave traveller the that\n"
     ]
    }
   ],
   "source": [
    "for seed in range(3):\n",
    "    np.random.seed(seed)\n",
    "    sonnet = [model(poLabel, 10) for _ in range(14)]\n",
    "    print(\"*******seed************************************\", seed)\n",
    "    print(\"\\n\".join(sonnet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9de4cd5-7145-4b8b-a5d1-cb41789f3fff",
   "metadata": {},
   "source": [
    "# Step 7: Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e79db91-41df-4cde-b8c7-357486aa255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e6abdf4-f4ae-4b0e-80a0-df09cdbb4fd8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__call__() missing 1 required positional argument: 'n'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [23], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m Y_test \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sen \u001b[38;5;129;01min\u001b[39;00m poTest:\n\u001b[1;32m----> 5\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pred \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m         pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: __call__() missing 1 required positional argument: 'n'"
     ]
    }
   ],
   "source": [
    "# build the confusion matrix\n",
    "Y_pred = []\n",
    "Y_test = []\n",
    "for sen in poTest:\n",
    "    pred = model(sen)\n",
    "    if pred is None:\n",
    "        pred = -1\n",
    "    Y_pred.append(pred)\n",
    "    Y_test.append(poLabel)\n",
    "    \n",
    "for sen in frostTest:\n",
    "    pred = model(sen)\n",
    "    if pred is None:\n",
    "        pred = -1\n",
    "    Y_pred.append(pred)\n",
    "    Y_test.append(frLabel)\n",
    "    \n",
    "cm = confusion_matrix(Y_test, Y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=[\"None\", \"Poe\", \"Frost\"])\n",
    "disp.plot()\n",
    "print(accuracy_score(Y_test, Y_pred), balanced_accuracy_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6cd2f2-44b5-4e9b-b686-eb583f389561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the confusion matrix\n",
    "Y_pred = []\n",
    "Y_test = []\n",
    "for sen in poTrain:\n",
    "    pred = model(sen)\n",
    "    if pred is None:\n",
    "        pred = -1\n",
    "    Y_pred.append(pred)\n",
    "    Y_test.append(poLabel)\n",
    "    \n",
    "for sen in frostTrain:\n",
    "    pred = model(sen)\n",
    "    if pred is None:\n",
    "        pred = -1\n",
    "    Y_pred.append(pred)\n",
    "    Y_test.append(frLabel)\n",
    "    \n",
    "cm = confusion_matrix(Y_test, Y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=[\"None\", \"Poe\", \"Frost\"])\n",
    "disp.plot()\n",
    "print(accuracy_score(Y_test, Y_pred), balanced_accuracy_score(Y_test, Y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4710edce-df96-4a6c-b1c8-577ad8cf3534",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
