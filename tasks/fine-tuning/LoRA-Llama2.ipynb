{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf7dd48b-adff-4bf4-8f92-9239497cc489",
   "metadata": {},
   "source": [
    "# https://pytorch.org/torchtune/stable/tutorials/lora_finetune.html\n",
    "LoRA memory savings come primarily from gradient and optimizer states, so if your modelâ€™s peak memory comes in its forward() method, then LoRA may not reduce peak memory.\n",
    "\n",
    "nn.Linear(in_dim,out_dim) layer could have rank as high as min(in_dim,out_dim)\n",
    "\n",
    "**The main idea:** Instead of updating weights of a layer, freeze the layer, and add a new low-rank-optimization layer and fine-tune it. \n",
    "**But:** Low-rank approximation of a matrix is a optimization problem. \n",
    "\n",
    "What does it do? Create a branch-network with lower number of parameters and train it. Then sum up the old outputs with new outputs! Training improves, inference does not!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf190cbb-f439-404b-9d11-cb37058dd047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67bbbfce-7901-40e1-8c8b-c0b721c79c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        out_dim: int,\n",
    "        rank: int,\n",
    "        alpha: float,\n",
    "        dropout: float\n",
    "    ):\n",
    "        # original pretrained layers\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.linear = nn.Linear(\n",
    "            self.in_dim, \n",
    "            self.out_dim, \n",
    "            bias=False\n",
    "        )\n",
    "        \n",
    "        # -----------------LoRA----------------\n",
    "        # new hyper parameters\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # new Kabab-me-haddi\n",
    "        self._addLoraLayers()\n",
    "\n",
    "        # now freeze the original model params\n",
    "        self._prepWeightsForFinetuning()\n",
    "\n",
    "    def _addLoraLayers(self):\n",
    "        self.lora_a = nn.Linear(\n",
    "            self.in_dim, \n",
    "            self.rank, \n",
    "            bias=False\n",
    "        )\n",
    "        self.lora_b = nn.Linear(\n",
    "            self.rank, \n",
    "            self.out_dim, \n",
    "            bias = False\n",
    "        )\n",
    "        # follow the convention\n",
    "        self.lora_dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def _prepWeightsForFinetuning(self):\n",
    "        self.linear.weight.requires_grad = False\n",
    "        self.lora_a.weight.requires_grad = True\n",
    "        self.lora_b.weight.requires_grad = True\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        frozen_out = self.linear(x)\n",
    "\n",
    "        lora_out = self.lora_b(\n",
    "            self.lora_a(\n",
    "                self.lora_dropout(x)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return frozen_out + (self.alpha / self.rank) * lora_out\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463c6351-55eb-4d9f-b8ef-2a909dc79317",
   "metadata": {},
   "source": [
    "## Optimizing Llama2 Q,K,V projection layers\n",
    "Self-attention in Llama2 has in_dim=out_dim=4096. So, each projection FFN has 4096x4096 = 16.7M parameters. With rank=8, we can reduce the number of trainable parameters of each projection to\n",
    "4096x8 + 8x4096 = 65K params!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f03799c-c417-49a3-a289-5c5ab46e7405",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adhocmaster\\anaconda3\\envs\\pytorch23\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Get the Llama2\n",
    "from  torchtune.models.llama2 import llama2_7b, lora_llama2_7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "908c9f84-2371-4d13-91fb-43915bcefaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseModel = llama2_7b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ddaabac-1f19-4e1b-a339-5dcd52f37fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default settings for lora_llama2_7b will match those for llama2_7b\n",
    "# We just need to define which layers we want LoRA applied to.\n",
    "# Within each self-attention, we can choose from [\"q_proj\", \"k_proj\", \"v_proj\", and \"output_proj\"].\n",
    "# We can also set apply_lora_to_mlp=True or apply_lora_to_output=True to apply LoRA to other linear\n",
    "# layers outside of the self-attention.\n",
    "loraModel = lora_llama2_7b(lora_attn_modules=[\"q_proj\", \"v_proj\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af47546-2504-4346-a41b-b7dc84f8521d",
   "metadata": {},
   "source": [
    "**Calling lora_llama_2_7b alone will not handle the definition of which parameters are trainable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c670b8a-6c6e-41f0-85a8-097d3ff9536d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CausalSelfAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (output_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (pos_embeddings): RotaryPositionalEmbeddings()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(baseModel.layers[0].attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d033cac-c158-425b-a613-cdf2e13ea409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CausalSelfAttention(\n",
      "  (q_proj): LoRALinear(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (lora_a): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (lora_b): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  )\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): LoRALinear(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (lora_a): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (lora_b): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  )\n",
      "  (output_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (pos_embeddings): RotaryPositionalEmbeddings()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(loraModel.layers[0].attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51ce2c94-f0f9-408b-9817-d025f408c28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['layers.0.attn.q_proj.lora_a.weight', 'layers.0.attn.q_proj.lora_b.weight', 'layers.0.attn.v_proj.lora_a.weight', 'layers.0.attn.v_proj.lora_b.weight', 'layers.1.attn.q_proj.lora_a.weight', 'layers.1.attn.q_proj.lora_b.weight', 'layers.1.attn.v_proj.lora_a.weight', 'layers.1.attn.v_proj.lora_b.weight', 'layers.2.attn.q_proj.lora_a.weight', 'layers.2.attn.q_proj.lora_b.weight', 'layers.2.attn.v_proj.lora_a.weight', 'layers.2.attn.v_proj.lora_b.weight', 'layers.3.attn.q_proj.lora_a.weight', 'layers.3.attn.q_proj.lora_b.weight', 'layers.3.attn.v_proj.lora_a.weight', 'layers.3.attn.v_proj.lora_b.weight', 'layers.4.attn.q_proj.lora_a.weight', 'layers.4.attn.q_proj.lora_b.weight', 'layers.4.attn.v_proj.lora_a.weight', 'layers.4.attn.v_proj.lora_b.weight', 'layers.5.attn.q_proj.lora_a.weight', 'layers.5.attn.q_proj.lora_b.weight', 'layers.5.attn.v_proj.lora_a.weight', 'layers.5.attn.v_proj.lora_b.weight', 'layers.6.attn.q_proj.lora_a.weight', 'layers.6.attn.q_proj.lora_b.weight', 'layers.6.attn.v_proj.lora_a.weight', 'layers.6.attn.v_proj.lora_b.weight', 'layers.7.attn.q_proj.lora_a.weight', 'layers.7.attn.q_proj.lora_b.weight', 'layers.7.attn.v_proj.lora_a.weight', 'layers.7.attn.v_proj.lora_b.weight', 'layers.8.attn.q_proj.lora_a.weight', 'layers.8.attn.q_proj.lora_b.weight', 'layers.8.attn.v_proj.lora_a.weight', 'layers.8.attn.v_proj.lora_b.weight', 'layers.9.attn.q_proj.lora_a.weight', 'layers.9.attn.q_proj.lora_b.weight', 'layers.9.attn.v_proj.lora_a.weight', 'layers.9.attn.v_proj.lora_b.weight', 'layers.10.attn.q_proj.lora_a.weight', 'layers.10.attn.q_proj.lora_b.weight', 'layers.10.attn.v_proj.lora_a.weight', 'layers.10.attn.v_proj.lora_b.weight', 'layers.11.attn.q_proj.lora_a.weight', 'layers.11.attn.q_proj.lora_b.weight', 'layers.11.attn.v_proj.lora_a.weight', 'layers.11.attn.v_proj.lora_b.weight', 'layers.12.attn.q_proj.lora_a.weight', 'layers.12.attn.q_proj.lora_b.weight', 'layers.12.attn.v_proj.lora_a.weight', 'layers.12.attn.v_proj.lora_b.weight', 'layers.13.attn.q_proj.lora_a.weight', 'layers.13.attn.q_proj.lora_b.weight', 'layers.13.attn.v_proj.lora_a.weight', 'layers.13.attn.v_proj.lora_b.weight', 'layers.14.attn.q_proj.lora_a.weight', 'layers.14.attn.q_proj.lora_b.weight', 'layers.14.attn.v_proj.lora_a.weight', 'layers.14.attn.v_proj.lora_b.weight', 'layers.15.attn.q_proj.lora_a.weight', 'layers.15.attn.q_proj.lora_b.weight', 'layers.15.attn.v_proj.lora_a.weight', 'layers.15.attn.v_proj.lora_b.weight', 'layers.16.attn.q_proj.lora_a.weight', 'layers.16.attn.q_proj.lora_b.weight', 'layers.16.attn.v_proj.lora_a.weight', 'layers.16.attn.v_proj.lora_b.weight', 'layers.17.attn.q_proj.lora_a.weight', 'layers.17.attn.q_proj.lora_b.weight', 'layers.17.attn.v_proj.lora_a.weight', 'layers.17.attn.v_proj.lora_b.weight', 'layers.18.attn.q_proj.lora_a.weight', 'layers.18.attn.q_proj.lora_b.weight', 'layers.18.attn.v_proj.lora_a.weight', 'layers.18.attn.v_proj.lora_b.weight', 'layers.19.attn.q_proj.lora_a.weight', 'layers.19.attn.q_proj.lora_b.weight', 'layers.19.attn.v_proj.lora_a.weight', 'layers.19.attn.v_proj.lora_b.weight', 'layers.20.attn.q_proj.lora_a.weight', 'layers.20.attn.q_proj.lora_b.weight', 'layers.20.attn.v_proj.lora_a.weight', 'layers.20.attn.v_proj.lora_b.weight', 'layers.21.attn.q_proj.lora_a.weight', 'layers.21.attn.q_proj.lora_b.weight', 'layers.21.attn.v_proj.lora_a.weight', 'layers.21.attn.v_proj.lora_b.weight', 'layers.22.attn.q_proj.lora_a.weight', 'layers.22.attn.q_proj.lora_b.weight', 'layers.22.attn.v_proj.lora_a.weight', 'layers.22.attn.v_proj.lora_b.weight', 'layers.23.attn.q_proj.lora_a.weight', 'layers.23.attn.q_proj.lora_b.weight', 'layers.23.attn.v_proj.lora_a.weight', 'layers.23.attn.v_proj.lora_b.weight', 'layers.24.attn.q_proj.lora_a.weight', 'layers.24.attn.q_proj.lora_b.weight', 'layers.24.attn.v_proj.lora_a.weight', 'layers.24.attn.v_proj.lora_b.weight', 'layers.25.attn.q_proj.lora_a.weight', 'layers.25.attn.q_proj.lora_b.weight', 'layers.25.attn.v_proj.lora_a.weight', 'layers.25.attn.v_proj.lora_b.weight', 'layers.26.attn.q_proj.lora_a.weight', 'layers.26.attn.q_proj.lora_b.weight', 'layers.26.attn.v_proj.lora_a.weight', 'layers.26.attn.v_proj.lora_b.weight', 'layers.27.attn.q_proj.lora_a.weight', 'layers.27.attn.q_proj.lora_b.weight', 'layers.27.attn.v_proj.lora_a.weight', 'layers.27.attn.v_proj.lora_b.weight', 'layers.28.attn.q_proj.lora_a.weight', 'layers.28.attn.q_proj.lora_b.weight', 'layers.28.attn.v_proj.lora_a.weight', 'layers.28.attn.v_proj.lora_b.weight', 'layers.29.attn.q_proj.lora_a.weight', 'layers.29.attn.q_proj.lora_b.weight', 'layers.29.attn.v_proj.lora_a.weight', 'layers.29.attn.v_proj.lora_b.weight', 'layers.30.attn.q_proj.lora_a.weight', 'layers.30.attn.q_proj.lora_b.weight', 'layers.30.attn.v_proj.lora_a.weight', 'layers.30.attn.v_proj.lora_b.weight', 'layers.31.attn.q_proj.lora_a.weight', 'layers.31.attn.q_proj.lora_b.weight', 'layers.31.attn.v_proj.lora_a.weight', 'layers.31.attn.v_proj.lora_b.weight'], unexpected_keys=[])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pretrained weights from the baseModel to the Lora model\n",
    "loraModel.load_state_dict(baseModel.state_dict(), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04ec1282-7ded-4171-915c-b040216cf6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting trainable parameters in the LoraModel\n",
    "from torchtune.modules.peft.peft_utils import get_adapter_params, set_trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f191ad13-561b-4e51-9b83-524b2bd703e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loraParams = get_adapter_params(loraModel)\n",
    "set_trainable_params(loraModel, loraParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7712cc6b-c2fa-4573-b05d-e8ffa1e308ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "totalParams = sum([p.numel() for p in loraModel.parameters()])\n",
    "trainableParams = sum([p.numel() for p in loraModel.parameters() if p.requires_grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd10712f-9551-4ff4-abb0-a8e69bf16ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  6742609920 total params,\n",
      "  4194304\" trainable params,\n",
      "  0.06% of all params are trainable.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "  f\"\"\"\n",
    "  {totalParams} total params,\n",
    "  {trainableParams}\" trainable params,\n",
    "  {(100.0 * trainableParams / totalParams):.2f}% of all params are trainable.\n",
    "  \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d08e62-2299-4cd4-afa4-361ae81c2c76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
